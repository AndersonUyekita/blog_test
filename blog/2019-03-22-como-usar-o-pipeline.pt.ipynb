{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Como usar o `Pipeline`\n",
    "\n",
    "#### Tags\n",
    "\n",
    "* Autor: AH Uyekita\n",
    "* Data: 2019/03/22\n",
    "* Post: [Como usar o `Pipeline`][post_url]\n",
    "\n",
    "[post_url]: https://andersonuyekita.github.io/notebooks/blog/como-usar-o-pipepline/\n",
    "\n",
    "#### Descrição\n",
    "\n",
    "Esse é um arquivo de acompanhamento do _post_ de mesmo nome.\n",
    "\n",
    "***\n",
    "\n",
    "### Índice\n",
    "- [1. Kernel](#1)\n",
    "- [2. Importação dos _Packages_](#2)\n",
    "- [3. _Datasets_](#3)\n",
    "- [4. _Classifiers_](#4)\n",
    "- [5. Uso do _Pipeline_](#5)\n",
    "    - [5.1 Parte 1](#5.1)\n",
    "    - [5.2 Parte 2](#5.2)\n",
    "    - [5.3 Encapsulamento do _Pipeline_](#5.3)\n",
    "- [6. _Pipeline para diversos Classifiers](#6)\n",
    "- [7. Conclusões](#7)\n",
    "- [8. Versões dos Packages](#8)\n",
    "\n",
    "***\n",
    "\n",
    "### 1. Kernel <a id='1'></a>\n",
    "\n",
    "Foi usado o Python 3.7, conforme pode ser confirmado abaixo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Importação dos _Packages_ básicos  <a id='2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação dos packages básicos.\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Só para evitar a aparição dos warnings.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. _Dataset_ <a id='3'></a>\n",
    "\n",
    "Será usado como exemplo de dataset para a aplicação do `Pipeline` o banco de dados de câncer de mama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando os dados de Câncer de mama.\n",
    "cancer = datasets.load_breast_cancer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O objeto `cancer` será um dicionário, dessa maneira vamos apenas carregar os dados que nos interessam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criação do dataset features e vetor labels.\n",
    "features = cancer.data\n",
    "labels = cancer.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. _Classifier_ <a id='4'></a>\n",
    "\n",
    "Esse exemplo terá como base a calibragem dos parâmetros do `Logistic Regression`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação do Logistic Regression.\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos usar o `constructor` do `Logistic Regression` para criar um objeto chamado `clf`, ora doravante _classifier_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uso do constructor do Logistic Regression para criar um classifier.\n",
    "clf = LogisticRegression(random_state = 42,\n",
    "                         solver = 'lbfgs',\n",
    "                         multi_class = 'multinomial')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Uso do Pipeline <a id='5'></a>\n",
    "\n",
    "A elucidação de como usar o Pipeline será exposto em duas partes:\n",
    "\n",
    "1. Fazendo passo-a-passo e explicando cada etapa, e;\n",
    "2. Encapsulando todas as etapas anteriores num Pipeline.\n",
    "\n",
    "#### 5.1. Parte 1 <a id='5.1'></a>\n",
    "\n",
    "Nessa etapa será feito cada etapa individualmente para um perfeito entendimento de cada uma delas e o seu papel no modelo.\n",
    "\n",
    "Analisemos os dados presentes em _features_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0      1       2       3        4        5       6        7       8   \\\n",
       "0  17.99  10.38  122.80  1001.0  0.11840  0.27760  0.3001  0.14710  0.2419   \n",
       "1  20.57  17.77  132.90  1326.0  0.08474  0.07864  0.0869  0.07017  0.1812   \n",
       "2  19.69  21.25  130.00  1203.0  0.10960  0.15990  0.1974  0.12790  0.2069   \n",
       "3  11.42  20.38   77.58   386.1  0.14250  0.28390  0.2414  0.10520  0.2597   \n",
       "4  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.1980  0.10430  0.1809   \n",
       "\n",
       "        9    ...        20     21      22      23      24      25      26  \\\n",
       "0  0.07871   ...     25.38  17.33  184.60  2019.0  0.1622  0.6656  0.7119   \n",
       "1  0.05667   ...     24.99  23.41  158.80  1956.0  0.1238  0.1866  0.2416   \n",
       "2  0.05999   ...     23.57  25.53  152.50  1709.0  0.1444  0.4245  0.4504   \n",
       "3  0.09744   ...     14.91  26.50   98.87   567.7  0.2098  0.8663  0.6869   \n",
       "4  0.05883   ...     22.54  16.67  152.20  1575.0  0.1374  0.2050  0.4000   \n",
       "\n",
       "       27      28       29  \n",
       "0  0.2654  0.4601  0.11890  \n",
       "1  0.1860  0.2750  0.08902  \n",
       "2  0.2430  0.3613  0.08758  \n",
       "3  0.2575  0.6638  0.17300  \n",
       "4  0.1625  0.2364  0.07678  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imprime as primeiras 5 linhas.\n",
    "pd.DataFrame(features).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observa-se a presença de 30 _features_ com diferentes não _ranges_, isso pode ser um problema, pois o `Logistic Regression` utiliza a distância entre os valores para guiar o _Gradient Descent_, logo é imprescindível a mudança de escala das variáveis.\n",
    "\n",
    "##### 5.1.1. Mudança de Escala\n",
    "\n",
    "Para isso será utilizado o `MinMaxScaler` (leia a documentação [aqui][minmaxscaler_url]).\n",
    "\n",
    "[minmaxscaler_url]: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação do MinMaxScaler.\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Constructor para criar o objeto do MinMaxScaler.\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Treino.\n",
    "scaler.fit(features)\n",
    "\n",
    "# Transformação.\n",
    "features_scaled = scaler.transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos imprimir os resultados para verificar a mudança de escala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.521037</td>\n",
       "      <td>0.022658</td>\n",
       "      <td>0.545989</td>\n",
       "      <td>0.363733</td>\n",
       "      <td>0.593753</td>\n",
       "      <td>0.792037</td>\n",
       "      <td>0.703140</td>\n",
       "      <td>0.731113</td>\n",
       "      <td>0.686364</td>\n",
       "      <td>0.605518</td>\n",
       "      <td>...</td>\n",
       "      <td>0.620776</td>\n",
       "      <td>0.141525</td>\n",
       "      <td>0.668310</td>\n",
       "      <td>0.450698</td>\n",
       "      <td>0.601136</td>\n",
       "      <td>0.619292</td>\n",
       "      <td>0.568610</td>\n",
       "      <td>0.912027</td>\n",
       "      <td>0.598462</td>\n",
       "      <td>0.418864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.643144</td>\n",
       "      <td>0.272574</td>\n",
       "      <td>0.615783</td>\n",
       "      <td>0.501591</td>\n",
       "      <td>0.289880</td>\n",
       "      <td>0.181768</td>\n",
       "      <td>0.203608</td>\n",
       "      <td>0.348757</td>\n",
       "      <td>0.379798</td>\n",
       "      <td>0.141323</td>\n",
       "      <td>...</td>\n",
       "      <td>0.606901</td>\n",
       "      <td>0.303571</td>\n",
       "      <td>0.539818</td>\n",
       "      <td>0.435214</td>\n",
       "      <td>0.347553</td>\n",
       "      <td>0.154563</td>\n",
       "      <td>0.192971</td>\n",
       "      <td>0.639175</td>\n",
       "      <td>0.233590</td>\n",
       "      <td>0.222878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.601496</td>\n",
       "      <td>0.390260</td>\n",
       "      <td>0.595743</td>\n",
       "      <td>0.449417</td>\n",
       "      <td>0.514309</td>\n",
       "      <td>0.431017</td>\n",
       "      <td>0.462512</td>\n",
       "      <td>0.635686</td>\n",
       "      <td>0.509596</td>\n",
       "      <td>0.211247</td>\n",
       "      <td>...</td>\n",
       "      <td>0.556386</td>\n",
       "      <td>0.360075</td>\n",
       "      <td>0.508442</td>\n",
       "      <td>0.374508</td>\n",
       "      <td>0.483590</td>\n",
       "      <td>0.385375</td>\n",
       "      <td>0.359744</td>\n",
       "      <td>0.835052</td>\n",
       "      <td>0.403706</td>\n",
       "      <td>0.213433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.210090</td>\n",
       "      <td>0.360839</td>\n",
       "      <td>0.233501</td>\n",
       "      <td>0.102906</td>\n",
       "      <td>0.811321</td>\n",
       "      <td>0.811361</td>\n",
       "      <td>0.565604</td>\n",
       "      <td>0.522863</td>\n",
       "      <td>0.776263</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.248310</td>\n",
       "      <td>0.385928</td>\n",
       "      <td>0.241347</td>\n",
       "      <td>0.094008</td>\n",
       "      <td>0.915472</td>\n",
       "      <td>0.814012</td>\n",
       "      <td>0.548642</td>\n",
       "      <td>0.884880</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.773711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.629893</td>\n",
       "      <td>0.156578</td>\n",
       "      <td>0.630986</td>\n",
       "      <td>0.489290</td>\n",
       "      <td>0.430351</td>\n",
       "      <td>0.347893</td>\n",
       "      <td>0.463918</td>\n",
       "      <td>0.518390</td>\n",
       "      <td>0.378283</td>\n",
       "      <td>0.186816</td>\n",
       "      <td>...</td>\n",
       "      <td>0.519744</td>\n",
       "      <td>0.123934</td>\n",
       "      <td>0.506948</td>\n",
       "      <td>0.341575</td>\n",
       "      <td>0.437364</td>\n",
       "      <td>0.172415</td>\n",
       "      <td>0.319489</td>\n",
       "      <td>0.558419</td>\n",
       "      <td>0.157500</td>\n",
       "      <td>0.142595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.521037  0.022658  0.545989  0.363733  0.593753  0.792037  0.703140   \n",
       "1  0.643144  0.272574  0.615783  0.501591  0.289880  0.181768  0.203608   \n",
       "2  0.601496  0.390260  0.595743  0.449417  0.514309  0.431017  0.462512   \n",
       "3  0.210090  0.360839  0.233501  0.102906  0.811321  0.811361  0.565604   \n",
       "4  0.629893  0.156578  0.630986  0.489290  0.430351  0.347893  0.463918   \n",
       "\n",
       "         7         8         9     ...           20        21        22  \\\n",
       "0  0.731113  0.686364  0.605518    ...     0.620776  0.141525  0.668310   \n",
       "1  0.348757  0.379798  0.141323    ...     0.606901  0.303571  0.539818   \n",
       "2  0.635686  0.509596  0.211247    ...     0.556386  0.360075  0.508442   \n",
       "3  0.522863  0.776263  1.000000    ...     0.248310  0.385928  0.241347   \n",
       "4  0.518390  0.378283  0.186816    ...     0.519744  0.123934  0.506948   \n",
       "\n",
       "         23        24        25        26        27        28        29  \n",
       "0  0.450698  0.601136  0.619292  0.568610  0.912027  0.598462  0.418864  \n",
       "1  0.435214  0.347553  0.154563  0.192971  0.639175  0.233590  0.222878  \n",
       "2  0.374508  0.483590  0.385375  0.359744  0.835052  0.403706  0.213433  \n",
       "3  0.094008  0.915472  0.814012  0.548642  0.884880  1.000000  0.773711  \n",
       "4  0.341575  0.437364  0.172415  0.319489  0.558419  0.157500  0.142595  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imprimir somente as primeiras 5 linhas das features_scaled.\n",
    "pd.DataFrame(features_scaled).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.1.2. Diminuição de Dimensões\n",
    "\n",
    "Devido à quantidade \"elevada\" de _features_, farei a reduação de 30 features para 10. Desta maneira, usarei o método de Componentes Principais para diminuir a dimensão do problema.\n",
    "\n",
    "Leia a documentação do PCA [aqui][pca_url].\n",
    "\n",
    "[pca_url]: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação do PCA.\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Uso do constructor para criar o objeto do PCA.\n",
    "pca = PCA(n_components = 10)\n",
    "\n",
    "# Treino.\n",
    "pca.fit(features_scaled)\n",
    "\n",
    "# Tranformação.\n",
    "features_scaled_pca = pca.transform(features_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos imprimir os resultados para verificar a nova quantidade de colunas (_features_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.387021</td>\n",
       "      <td>0.426895</td>\n",
       "      <td>-0.541703</td>\n",
       "      <td>0.048483</td>\n",
       "      <td>-0.072198</td>\n",
       "      <td>0.190817</td>\n",
       "      <td>0.236313</td>\n",
       "      <td>-0.039456</td>\n",
       "      <td>0.077590</td>\n",
       "      <td>0.155295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.462308</td>\n",
       "      <td>-0.556947</td>\n",
       "      <td>-0.205175</td>\n",
       "      <td>-0.042830</td>\n",
       "      <td>0.016111</td>\n",
       "      <td>0.015604</td>\n",
       "      <td>0.043139</td>\n",
       "      <td>0.020644</td>\n",
       "      <td>-0.070639</td>\n",
       "      <td>-0.085284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.954621</td>\n",
       "      <td>-0.109701</td>\n",
       "      <td>-0.147848</td>\n",
       "      <td>-0.001068</td>\n",
       "      <td>-0.033798</td>\n",
       "      <td>0.069062</td>\n",
       "      <td>-0.108166</td>\n",
       "      <td>0.007362</td>\n",
       "      <td>-0.059335</td>\n",
       "      <td>-0.073689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000816</td>\n",
       "      <td>1.525089</td>\n",
       "      <td>-0.053271</td>\n",
       "      <td>-0.207916</td>\n",
       "      <td>-0.219381</td>\n",
       "      <td>0.388007</td>\n",
       "      <td>0.194519</td>\n",
       "      <td>0.143499</td>\n",
       "      <td>0.176996</td>\n",
       "      <td>-0.140951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.626828</td>\n",
       "      <td>-0.302471</td>\n",
       "      <td>-0.409336</td>\n",
       "      <td>0.238811</td>\n",
       "      <td>-0.002192</td>\n",
       "      <td>-0.157212</td>\n",
       "      <td>-0.063308</td>\n",
       "      <td>0.045931</td>\n",
       "      <td>0.002422</td>\n",
       "      <td>0.000545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  1.387021  0.426895 -0.541703  0.048483 -0.072198  0.190817  0.236313   \n",
       "1  0.462308 -0.556947 -0.205175 -0.042830  0.016111  0.015604  0.043139   \n",
       "2  0.954621 -0.109701 -0.147848 -0.001068 -0.033798  0.069062 -0.108166   \n",
       "3  1.000816  1.525089 -0.053271 -0.207916 -0.219381  0.388007  0.194519   \n",
       "4  0.626828 -0.302471 -0.409336  0.238811 -0.002192 -0.157212 -0.063308   \n",
       "\n",
       "          7         8         9  \n",
       "0 -0.039456  0.077590  0.155295  \n",
       "1  0.020644 -0.070639 -0.085284  \n",
       "2  0.007362 -0.059335 -0.073689  \n",
       "3  0.143499  0.176996 -0.140951  \n",
       "4  0.045931  0.002422  0.000545  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imprime as primeiras 5 linhas das features após PCA e scale.\n",
    "pd.DataFrame(features_scaled_pca).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.1.3. Criação de um Classifier\n",
    "\n",
    "Com base no _classfier_ (`clf`) criado no item 4, vamos fazer previsões.\n",
    "\n",
    "Primeiramente, vamos dividir o dataset em treino e teste para não incorrer em _overfitting_. O motivo de usar o `train_test_split` é simplesmente para simplificar o modelo e tornar o entendimento mais fácil.\n",
    "\n",
    "Leia sobre o `train_test_spit` [aqui][train_test_split_url]\n",
    "\n",
    "[train_test_split_url]: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação do StratifiedShuffleSplit.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Divisão dos features e labels em treino e teste.\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features_scaled_pca,\n",
    "                                                                            labels,\n",
    "                                                                            test_size=0.33,\n",
    "                                                                            random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que já foram criados os datasets de treino e teste, pode-se treinar e calcular as métricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9787234042553191"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Treino usando o dataset de treino.\n",
    "clf.fit(features_train, labels_train)\n",
    "\n",
    "# Previsões usando o dataset de test.\n",
    "pred = clf.predict(features_test)\n",
    "\n",
    "# Importação do módulo para cálculo do Accuracy.\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Avaliação.\n",
    "accuracy_score(labels_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após três etapas (5.1.1, 5.1.2 e 5.1.3) foi possível calcular o _accuracy_ do modelo. \n",
    "\n",
    ">Obs.: Não entraremos no mérito se os parâmetros da `Logistic Regression` são os melhores, pois esse não é o objetivo deste arquivo. A idéia aqui é analisar o _workflow_ da construção do modelo e como usar o `Pipeline` para automatizar parte disso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2. Parte 2 <a id='5.2'></a>\n",
    "\n",
    "Após o pleno entendimento da Parte 1, onde foi feito passo-a-passo de cada etapa a Parte 2 tem como objetivo **encapsular** as três etapas elucidadas nos itens: 5.1.1, 5.1.2, e 5.1.3 em uma etapa só. Isso será feito usando o `Pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação do Pipeline\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sequência de etapas até agora foi:\n",
    "\n",
    "$$\\text{Minhas Etapas = [Mudança de Escala, Diminuição de Dimensão, Previsão]}$$\n",
    "\n",
    "O `Pipeline` necessita que seja criado uma lista de etapas, portanto será definido abaixo a lista de etapas com os seus respectivos \"nomes\" e **objetos**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minha lista de etapas.\n",
    "etapas_pipeline = [('scaler', scaler), # O scaler está definido na Etapa 5.1.1\n",
    "                   ('pca'   , pca),    # O pca está definido na Etapa 5.1.2\n",
    "                   ('clf'   , clf)]    # O clf está definido no item 4 e foi usado na Etapa 5.1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todavia, pode-se substituir o **objeto** pelo **constructor**.\n",
    "\n",
    "$$\\text{('nome_da_etapa', constructor)}$$\n",
    "\n",
    "Sendo assim uma generalização das nossas etapas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minha lista de etapas.\n",
    "etapas_pipeline = [('scaler', MinMaxScaler()),                                     # Etapa 5.1.1\n",
    "                   ('pca'   , PCA(n_components = 10)),                             # Etapa 5.1.2\n",
    "                   ('clf'   , LogisticRegression(random_state = 42,                # Etapa 5.1.3\n",
    "                                                 solver = 'lbfgs',\n",
    "                                                 multi_class = 'multinomial'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por fim, tem-se que todas as etapas foram definidas e já é possível usar o _constructor_ do `Pipeline` para criar o objeto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uso do constructor para criação do objeto do Pipeline.\n",
    "pipe = Pipeline(etapas_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que agora usaremos o objeto `pipe` como se ele fosse um classifier qualquer. Desta maneira, ele será treinado e depois usado para fazer previsões.\n",
    "\n",
    "Analogamente à Etapa 1, será usado o simples `train_test_split` por ser o mais simples e de fácil entendimento. Observe que foi usado o **features original** sem modificações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divisão dos features e labels em treino e teste.\n",
    "features_train_2, features_test_2, labels_train_2, labels_test_2 = train_test_split(features,\n",
    "                                                                                    labels,\n",
    "                                                                                    test_size=0.33,\n",
    "                                                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treina-se o `pipe` usando o par de treino (features de treino e labels de treino)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treino do pipe.\n",
    "pipe.fit(features_train_2, labels_train_2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faz-se as previsões usando o `pipe` e o método `predict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazendo previsões.\n",
    "pred_2 = pipe.predict(features_test_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora podemos avaliar os resultaos e esperar que sejam iguais aos valores obtidos na Etapa 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9787234042553191"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    # Calculando o accuracy.\n",
    "    accuracy_score(labels_test_2, pred_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que os resultados são iguais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comparação entre os resultados das duas etapas. \n",
    "accuracy_score(labels_test_2, pred_2) == accuracy_score(labels_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que o `Pipeline` aplica **ocultamente** os processos `fit` e `transform` das primeiras duas etapas, deixando apenas a parte de `fit` e `predict` do _classifier_. Logo, é um **requisito necessário** para que o `Pipeline` funcione que as etapas que antecedem o _classifier_ possuam esses dois métodos.\n",
    "\n",
    ">**Não há como ter dois _classifiers_ como etapas do Pipeline. Só admite um _classifier_ e ele deve ser a última etapa.**\n",
    "\n",
    "O motivo é simples: Os _classifiers_ geralmente não possuem o método `transform`, o que é um requisito do `Pipeline`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Encapsulamento do Pipeline <a id='5.3'></a>\n",
    "\n",
    "O `Pipeline` pode ter o seu uso ampliado ao criar uma função, conforme exemplificado abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meu_pipeline(n_components = 10, solver = 'lbgs', multi_class = 'multinomial',\n",
    "                 features = features, labels = labels, test_size = 0.33):\n",
    "    \"\"\"\n",
    "    Este é um exemplo bem simples de como encapsular o Pipeline numa função.\n",
    "    Note que essa função oferece a possibilidade de teste de diversos parâmetros.\n",
    "    \"\"\"\n",
    "    # Divisão dos features e labels em treino e teste.\n",
    "    features_train_3, features_test_3, labels_train_3, labels_test_3 = train_test_split(features,\n",
    "                                                                                        labels,\n",
    "                                                                                        test_size = test_size,\n",
    "                                                                                        random_state = 42)\n",
    "    \n",
    "    # Minha lista de etapas.\n",
    "    etapas_pipeline = [('scaler', MinMaxScaler()),\n",
    "                       ('pca'   , PCA(n_components = n_components)),\n",
    "                       ('clf'   , LogisticRegression(random_state = 42,\n",
    "                                                     solver = solver,\n",
    "                                                     multi_class = multi_class))]\n",
    "    \n",
    "    # Treinando.\n",
    "    pipe.fit(features_train_3, labels_train_3)\n",
    "    \n",
    "    # Prevendo.\n",
    "    pred_3 = pipe.predict(features_test_3)\n",
    "    \n",
    "    # Comparação entre os resultados das duas etapas. \n",
    "    acc = accuracy_score(labels_test_3, pred_3)\n",
    "    \n",
    "    # Retorno do accuracy.\n",
    "    return(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos testar a função usando os valores utilizados nas simulações anteriores e comparando o resultado final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9787234042553191"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uso da função usando valores default.\n",
    "meu_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fazendo um teste com um número diferente de `test_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9824561403508771"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculando o Accuracy para um tamanho de teste de 20%.\n",
    "meu_pipeline(test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Portanto, o uso do `Pipeline` não altera em nada os resultados (desde que o algoritmo seja [**determinístico**][alg_deterministico]) e possui o benefício de proporcionar **menos** linhas de códigos.\n",
    "\n",
    "[alg_deterministico]: https://pt.wikipedia.org/wiki/Algoritmo_determinístico\n",
    "\n",
    "Os benefícios de um código mais enxuto (como esse da função):\n",
    "\n",
    "* Estará menos suscetível a erro humano;\n",
    "* Mais fácil de fazer a manutenção.\n",
    "\n",
    "Em contra partida haverá:\n",
    "\n",
    "* Maior abstração."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Pipeline para diversos _Classifiers_ (Técnicas \"Avançadas\") <a id='6'></a>\n",
    "\n",
    "Uma maneira simples de utilizar vários _classifiers_ numa mesma estrutura de `Pipeline` é o uso de um segundo dicionário. A estrutura desse dicionário é a seguinte:\n",
    "\n",
    "$$\\text{meus_classifiers = \\{'chave_1' : ('nome_1', constructor_1), } \\\\ \\text{'chave_2' : ('nome_2', constructor_2)\\}}$$\n",
    "\n",
    "Vamos para um exemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação do Naïve Bayes.\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Dicionário de classifiers.\n",
    "meus_classifiers = {'logit':('lr', LogisticRegression(random_state = 42,\n",
    "                                                      solver = 'lbfgs',\n",
    "                                                      multi_class = 'multinomial')),\n",
    "                    'bayes':('gnb', GaussianNB())}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Como isso funciona?\n",
    "\n",
    "Ao definir a `chave` o dicionário retornará um _tuple_. Lembre-se que a lista de etapas do `Pipeline` deve ser composta de _tuples_.\n",
    "\n",
    "Vamos ao teste! Usemos o Naïve Bayes cujo `key` é `bayes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gnb', GaussianNB(priors=None, var_smoothing=1e-09))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imprime um tuple. Será o nome do objeto após o uso do constructor.\n",
    "meus_classifiers['bayes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A idéia é inserir esse dicionário dentro de outro que já conhecemos, o `etapas_pipeline`, que reproduzo abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minha lista de etapas.\n",
    "etapas_pipeline = [('scaler', MinMaxScaler()),\n",
    "                   ('pca'   , PCA(n_components = 10)),\n",
    "                   ('clf'   , LogisticRegression(random_state = 42,\n",
    "                                                 solver = 'lbfgs',\n",
    "                                                 multi_class = 'multinomial'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos substituir a última etapa da lista `etapa_pipeline` pelo nosso dicionário de _classifiers_ e usar `logit` como `key`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minha lista de etapas com um dicionário aninhado.\n",
    "etapas_pipeline_2 = [('scaler', MinMaxScaler()),\n",
    "                     ('pca'   , PCA(n_components = 10)),\n",
    "                      meus_classifiers['logit']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que se deve definir o `key`, caso contrário o Pipeline não conseguirá entender a lista de etapas, pois ele espera sempre receber uma lista composta por _tuples_.\n",
    "\n",
    "Calculemos o _accuracy_ usando a lista `etapas_pipeline_2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9840425531914894"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uso do constructor para criação do objeto do Pipeline.\n",
    "pipe_2 = Pipeline(etapas_pipeline_2)\n",
    "\n",
    "# Treino do pipe.\n",
    "pipe_2.fit(features_train_2, labels_train_2);\n",
    "\n",
    "# Fazendo previsões.\n",
    "pred_4 = pipe.predict(features_test_2)\n",
    "\n",
    "# Calculando o accuracy.\n",
    "accuracy_score(labels_test_2, pred_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que se pode eliminar a lista de etapas e definí-la diretamente dentro do `Pipeline` conforme o exemplo abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9202127659574468"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uso do constructor para criação do objeto do Pipeline.\n",
    "pipe_3 = Pipeline([('scaler', MinMaxScaler()),\n",
    "                   ('pca'   , PCA(n_components = 10)),\n",
    "                    meus_classifiers['bayes']])         # Estou usando o Naïve Bayes agora.\n",
    "\n",
    "# Treino do pipe.\n",
    "pipe_3.fit(features_train_2, labels_train_2);\n",
    "\n",
    "# Fazendo previsões.\n",
    "pred_5 = pipe_3.predict(features_test_2)\n",
    "\n",
    "# Calculando o accuracy.\n",
    "accuracy_score(labels_test_2, pred_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim como foi feito no item 5.3 é possível o encapsulamento desse excerto de código acima numa função."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Conclusões <a id='7'></a>\n",
    "\n",
    "O `Pipeline` pode automatizar grande parte do processo de calibragem do modelo e ser benéfico ao _workflow_, mas ele também pode ser usado de forma errada quando se adiciona etapas sem uma análise crítica. **Um exemplo é o que foi feito nesse arquivo**, a etapa de mudança de escala das _features_ foi feito sempre que o `Pipeline` foi executado.\n",
    "\n",
    "Observe que convenientemente deve-se fazer a mudança de escala para otimizar o processo do _Gradient Descent_ dos algoritmos que são baseados em distância, logo de forma geral é muito aconselhável fazer a mudança de escala para todos os algoritmos, pois não há nenhuma desvantagem em mudar a escala das _features_. Portanto, a remoção do `MinMaxScaler` da lista de etapas e executando ele antes do `Pipeline` resultará num algoritmo mais eficiente, pois ele será executado uma vez só.\n",
    "\n",
    "Já para o caso do `PCA` é conveniente que se deixe dentro do `Pipeline`, pois dará a oportunidade de variarmos a quantidade de componentes principais (`n_components`).\n",
    "\n",
    "Por fim, tenha em mente que para _datasets_ pequenos o tempo de execução não é algo a se preocupar, mas em casos reais onde o tempo é um fator limitante, eliminar repetições desnecessárias é fundamental para otimizar os gastos dos seus recursos (tempo, dinheiro, esforço, paciência, etc.).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Versões <a id='8'></a>\n",
    "\n",
    "Usei o [sinfo][sinfo_url] para imprimir as versões de cada módulo e package utilizado neste _Jupyter Notebook_.\n",
    "\n",
    "[sinfo_url]: https://gitlab.com/joelostblom/sinfo\n",
    "\n",
    "```\n",
    "# No terminal/Prompt.\n",
    "pip install sinfo\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "numpy     \t1.15.4\n",
      "pandas    \t0.23.4\n",
      "sklearn   \t0.20.2\n",
      "-----\n",
      "IPython   \t7.2.0\n",
      "jupyter_client\t5.2.4\n",
      "jupyter_core\t4.4.0\n",
      "notebook  \t5.7.4\n",
      "-----\n",
      "Python 3.7.1 (v3.7.1:260ec2c36a, Oct 20 2018, 14:05:16) [MSC v.1915 32 bit (Intel)]\n",
      "Windows-10-10.0.17763-SP0\n",
      "4 logical CPU cores, Intel64 Family 6 Model 61 Stepping 4, GenuineIntel\n",
      "-----\n",
      "Session information updated at 2019-03-23 04:18\n"
     ]
    }
   ],
   "source": [
    "# Importa o sinfo.\n",
    "import sinfo\n",
    "\n",
    "# Imprime as versões dos Packages.\n",
    "sinfo.sinfo()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
